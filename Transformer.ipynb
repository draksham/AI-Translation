{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a5dee5",
      "metadata": {
        "id": "62a5dee5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17027ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17027ed",
        "outputId": "4ba628d9-ed3b-40af-8ef7-ef657115dafb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(42)  # For reproducibility\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffad93f9",
      "metadata": {
        "id": "ffad93f9"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "$$\n",
        "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
        "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf156177",
      "metadata": {
        "id": "bf156177"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"The multi-head attention module\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure the dimension of the model is divisible by the number of heads.\n",
        "        # This is necessary to equally divide the embedding dimension across heads.\n",
        "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "\n",
        "        self.d_model = d_model           # Total dimension of the model\n",
        "        self.num_heads = num_heads       # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # 1. Calculate attention scores with scaling\n",
        "        # 2. Apply mask (if provided) by setting masked positions to a large negative value\n",
        "        # 3. Apply softmax to attention scores to get probabilities\n",
        "        # 4. Return the weighted sum of values based on attention probabilities\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        prob_attn = F.softmax(scores, dim=-1)\n",
        "        output =  torch.matmul(prob_attn, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
        "        # to prepare for multi-head attention processing\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
        "        # [batch_size, seq_length, d_model]\n",
        "        batch_size, num_heads, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
        "        # 2. Apply scaled dot-product attention for each head\n",
        "        # 3. Concatenate the heads' outputs and apply the final linear projection\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # apply same mask to all `num_heads` heads.\n",
        "        # if mask is not None:\n",
        "        #     mask = mask.unsqueeze(1)#.repeat(1, self.num_heads, 1, 1)\n",
        "\n",
        "        # print(f'mask shape: {mask.shape}')\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        # print(f'attn_output shape: {attn_output.shape}')\n",
        "        attn_output = self.combine_heads(attn_output)\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b13867",
      "metadata": {
        "id": "70b13867"
      },
      "source": [
        "### Feed-Forward NN\n",
        "\n",
        "$$\n",
        "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5ab551",
      "metadata": {
        "id": "5d5ab551"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4034ef0a",
      "metadata": {
        "id": "4034ef0a"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "$$\n",
        "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
        "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfca835c",
      "metadata": {
        "id": "bfca835c"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding module using sinusoidal functions of different frequencies\n",
        "    for each dimension of the encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
        "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the division term used in the formulas for sin and cos functions.\n",
        "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
        "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
        "        # multiplying the position by the division term, creating a pattern where each position has\n",
        "        # a unique sinusoidal encoding.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
        "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
        "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor x.\n",
        "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
        "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
        "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e2475b",
      "metadata": {
        "id": "96e2475b"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000313c6",
      "metadata": {
        "id": "000313c6"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # Self-attention sublayer with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Feedforward sublayer with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76215e02",
      "metadata": {
        "id": "76215e02"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee27007e",
      "metadata": {
        "id": "ee27007e"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # Self-attention sublayer with residual connection and layer normalization\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = x + self.dropout(self_attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Cross-attention sublayer with residual connection and layer normalization\n",
        "        # query=decoder input, key and value=encoder output\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = x + self.dropout(cross_attn_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        # Feedforward sublayer with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad86614",
      "metadata": {
        "id": "2ad86614"
      },
      "source": [
        "### The Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31af1f2a",
      "metadata": {
        "id": "31af1f2a"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers for source and target\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialization\n",
        "        self.init_weights()\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_source_mask(self, src):\n",
        "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n",
        "        # Source padding mask\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(2) adds a dimension for the attention scores\n",
        "        # This mask can be broadcasted across the src_len dimension of the attention scores,\n",
        "        # effectively masking out specific tokens across all heads and all positions in the sequence.\n",
        "        return src_mask\n",
        "\n",
        "    def create_target_mask(self, tgt):\n",
        "        # Target padding mask\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(3) adds a dimension for the attention scores\n",
        "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n",
        "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
        "\n",
        "        # Target no-peak mask\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src):\n",
        "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
        "        \"\"\"\n",
        "        src_mask = self.create_source_mask(src)\n",
        "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
        "\n",
        "        # Pass through each layer in the encoder\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "        return src, src_mask\n",
        "\n",
        "    def decode(self, tgt, memory, src_mask):\n",
        "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
        "        \"\"\"\n",
        "        tgt_mask = self.create_target_mask(tgt)\n",
        "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
        "\n",
        "        # Pass through each layer in the decoder\n",
        "        for layer in self.decoder:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.out(tgt)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Encode the source sequence\n",
        "        memory, src_mask = self.encode(src)\n",
        "\n",
        "        # Decode the target sequence\n",
        "        output = self.decode(tgt, memory, src_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a3b60d",
      "metadata": {
        "id": "11a3b60d"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = 5000  # Size of source vocabulary\n",
        "tgt_vocab_size = 5000  # Size of target vocabulary\n",
        "d_model = 512          # Embedding dimension\n",
        "N = 6                  # Number of encoder and decoder layers\n",
        "num_heads = 8          # Number of attention heads\n",
        "d_ff = 2048            # Dimension of feed forward networks\n",
        "max_seq_length = 100   # Maximum sequence length\n",
        "dropout = 0.1          # Dropout rate\n",
        "pad_idx = 0            # Index of the padding token\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab4a81d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab4a81d",
        "outputId": "c9eebd05-12c8-4109-f96c-383b8ccf83bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_embedding): Embedding(5000, 512)\n",
              "  (tgt_embedding): Embedding(5000, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): ModuleList(\n",
              "    (0): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=5000, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050f7248",
      "metadata": {
        "id": "050f7248"
      },
      "source": [
        "### Testing on Random Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "149dadd1",
      "metadata": {
        "id": "149dadd1"
      },
      "outputs": [],
      "source": [
        "# Generate random sample data\n",
        "torch.manual_seed(42)\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5b1c56",
      "metadata": {
        "id": "2d5b1c56"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45583975",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45583975",
        "outputId": "65261fe9-85e2-4be5-e6ef-006b374bf0b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([990], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Generate the next token using the first token in the first target tensor\n",
        "model.eval()\n",
        "\n",
        "memory, src_mask = model.encode(src_data[:1, :])\n",
        "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
        "y = output.view(-1, tgt_vocab_size).argmax(-1)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b7e1c5",
      "metadata": {
        "id": "18b7e1c5"
      },
      "source": [
        "If your code is correct, you should get tensor([990])."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a314148e",
      "metadata": {
        "id": "a314148e"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afd8ff3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afd8ff3",
        "outputId": "c809462b-0cf3-4264-fb80-c6b7768f4bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.605189323425293\n",
            "Epoch: 2, Loss: 8.501504898071289\n",
            "Epoch: 3, Loss: 8.371415138244629\n",
            "Epoch: 4, Loss: 8.29697322845459\n",
            "Epoch: 5, Loss: 8.23850154876709\n",
            "Epoch: 6, Loss: 8.192160606384277\n",
            "Epoch: 7, Loss: 8.164847373962402\n",
            "Epoch: 8, Loss: 8.14220905303955\n",
            "Epoch: 9, Loss: 8.130326271057129\n",
            "Epoch: 10, Loss: 8.121962547302246\n"
          ]
        }
      ],
      "source": [
        "# Train the model for 10 epochs\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "model.train()\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src_data, tgt_data[:, :-1])\n",
        "\n",
        "    # tgt_data is of shape [batch_size, tgt_len]\n",
        "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
        "    loss = criterion(output, tgt)\n",
        "\n",
        "    loss.backward()\n",
        "    # nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f36f48",
      "metadata": {
        "id": "42f36f48"
      },
      "source": [
        "You should see the loss decreasing from around 8.6 to 8.1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71caa7ed",
      "metadata": {
        "id": "71caa7ed"
      },
      "source": [
        "### Machine Translation Example\n",
        "\n",
        "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. <br>\n",
        "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a17e60",
      "metadata": {
        "id": "a9a17e60"
      },
      "outputs": [],
      "source": [
        "# To run this example make sure you have the following packages installed:\n",
        "# NOTE: Reload Colab session after installing\n",
        "%pip install spacy torchtext portalocker --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38e67f4",
      "metadata": {
        "id": "c38e67f4"
      },
      "source": [
        "#### Define Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f20abd",
      "metadata": {
        "id": "08f20abd"
      },
      "outputs": [],
      "source": [
        "# Load spacy models for tokenization\n",
        "try:\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download de_core_news_sm\")\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer, language):\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer(data_sample[language])\n",
        "\n",
        "tokenizer_de = get_tokenizer(tokenize_de)\n",
        "tokenizer_en = get_tokenizer(tokenize_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1c9820",
      "metadata": {
        "id": "0e1c9820"
      },
      "source": [
        "#### Build Vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7474b5e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7474b5e6",
        "outputId": "0aac701f-172a-4bdc-a37c-ac09407afb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.8.2)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0->torchdata) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary packages\n",
        "!pip install torchdata\n",
        "!pip install torchtext\n",
        "!pip install spacy\n",
        "\n",
        "# Download the spacy models\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9E3jKjEyTeI",
        "outputId": "bfe386f6-4907-4923-f9ae-c3131b485309"
      },
      "id": "D9E3jKjEyTeI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.8.2)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0->torchdata) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.2.2)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0->torchtext) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchtext torchdata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxyZm9es4E2S",
        "outputId": "06d6c204-c67e-449a-ffd4-a7b027a4cb46"
      },
      "id": "FxyZm9es4E2S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.12.0\n",
            "Uninstalling torch-1.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/caffe2/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-1.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch-1.12.0\n",
            "Found existing installation: torchtext 0.13.0\n",
            "Uninstalling torchtext-0.13.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext-0.13.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.0 torchtext==0.13.0 torchdata==0.4.0\n"
      ],
      "metadata": {
        "id": "h17grrW54R_s"
      },
      "id": "h17grrW54R_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.0 torchtext==0.13.0 torchdata==0.4.0 spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "pJcxsYUd6DS-"
      },
      "id": "pJcxsYUd6DS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "914ca38f",
      "metadata": {
        "id": "914ca38f"
      },
      "source": [
        "#### Create the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c862170",
      "metadata": {
        "id": "4c862170"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters for the training process\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa58a6e",
      "metadata": {
        "id": "0aa58a6e"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca10d94",
      "metadata": {
        "id": "3ca10d94"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_data_iter):\n",
        "    data = []\n",
        "    for raw_src, raw_tgt in raw_data_iter:\n",
        "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
        "        data.append((src_tensor, tgt_tensor))\n",
        "    return data\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
        "train_data = data_process(train_data)\n",
        "valid_data = data_process(valid_data)\n",
        "#test_data = data_process(test_data)\n",
        "# The test set of Multi30k is corrupted\n",
        "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2f452a",
      "metadata": {
        "id": "1c2f452a"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch):\n",
        "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
        "    to each sequence and padding all sequences to the same length.\n",
        "\n",
        "    Parameters:\n",
        "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
        "      containing the source sequence tensor and the target sequence tensor.\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    # Iterate over each source-target pair in the provided batch\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n",
        "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n",
        "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n",
        "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
        "\n",
        "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
        "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
        "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbd8431",
      "metadata": {
        "id": "2cbd8431"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, grad_clip):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch over the given dataset.\n",
        "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
        "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - iterator (Iterable): An iterable object that returns batches of data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
        "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
        "    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode.\n",
        "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Enumerate over the data iterator to get batches\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model.\n",
        "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Reshape the output and target tensors to compute loss.\n",
        "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
        "\n",
        "        # tgt is of shape [batch_size, tgt_len]\n",
        "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "\n",
        "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss, perform backpropagation, and update model parameters\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Compute average loss per batch for the current epoch\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d18844",
      "metadata": {
        "id": "31d18844"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "    This function is similar to the training loop, but without the backward pass and parameter updates. I\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0287e5dd",
      "metadata": {
        "id": "0287e5dd"
      },
      "source": [
        "#### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02463b8",
      "metadata": {
        "id": "d02463b8"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a844bb",
      "metadata": {
        "id": "64a844bb"
      },
      "source": [
        "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save trained model\n",
        "torch.save(model.state_dict(), 'transformer_model.pt')"
      ],
      "metadata": {
        "id": "Y8AgPDre_fa-"
      },
      "id": "Y8AgPDre_fa-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ITSyy_N2_tMk"
      },
      "id": "ITSyy_N2_tMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp transformer_model.pt /content/drive/MyDrive/transformer_model.pt"
      ],
      "metadata": {
        "id": "uDhMYRzrAHEN"
      },
      "id": "uDhMYRzrAHEN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "817a7300",
      "metadata": {
        "id": "817a7300"
      },
      "source": [
        "#### Translating a Sample Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1bd36c",
      "metadata": {
        "id": "0b1bd36c"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_width=5):\n",
        "    model.eval()\n",
        "    tokens = tokenizer_de(sentence)\n",
        "    src_indexes = [vocab_src['<bos>']] + [vocab_src[token] for token in tokens] + [vocab_src['<eos>']]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    # Initialize beams as a list of tuples (score, [token indexes list])\n",
        "    beams = [(0, [vocab_tgt['<bos>']])]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for beam in beams:\n",
        "            trg_tensor = torch.LongTensor(beam[1]).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model.decode(trg_tensor, memory, src_mask)\n",
        "                output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "            # Consider the last token for further expansion\n",
        "            log_probs, next_tokens = output[0, -1].topk(beam_width)\n",
        "\n",
        "            for log_prob, next_token in zip(log_probs, next_tokens):\n",
        "                candidate = (beam[0] + log_prob.item(), beam[1] + [next_token.item()])\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Order all candidates by score\n",
        "        all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "        # Select the top beam_width candidates\n",
        "        beams = all_candidates[:beam_width]\n",
        "\n",
        "        # Break if the top beam candidate ends with <eos>\n",
        "        if beams[0][1][-1] == vocab_tgt['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Choose the best beam after completion\n",
        "    best_beam = beams[0][1]\n",
        "\n",
        "    # Convert the best beam's token indices to text\n",
        "    trg_tokens = [vocab_tgt.lookup_token(i) for i in best_beam]\n",
        "    translated_sentence = ' '.join(trg_tokens[1:])  # Skip <bos>\n",
        "\n",
        "    return translated_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438aff1f",
      "metadata": {
        "id": "438aff1f"
      },
      "outputs": [],
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6be352",
      "metadata": {
        "id": "ec6be352"
      },
      "source": [
        "You should get a translation similar to the reference after 20 epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ],
      "metadata": {
        "id": "rUH9ep7xHTXk"
      },
      "id": "rUH9ep7xHTXk"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_beam_search(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_width=5):\n",
        "    model.eval()\n",
        "    tokens = tokenizer_de(sentence)  # Replace tokenizer_de with actual tokenization\n",
        "    src_indexes = [vocab_src['<bos>']] + [vocab_src[token] for token in tokens] + [vocab_src['<eos>']]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    beams = [(0, [vocab_tgt['<bos>']])]  # Initialize beams with score and starting token\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for score, trg_indexes in beams:\n",
        "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model.decode(trg_tensor, memory, src_mask)\n",
        "                output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "            log_probs, tokens = output[:, -1, :].topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                next_token = tokens[0, i].item()\n",
        "                next_score = score + log_probs[0, i].item()\n",
        "                candidate = (next_score, trg_indexes + [next_token])\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "        if any([cand[1][-1] == vocab_tgt['<eos>'] for cand in beams]):\n",
        "            break\n",
        "\n",
        "    best_beam = beams[0][1]\n",
        "    translated_sentence = ' '.join([vocab_tgt.lookup_token(i) for i in best_beam[1:]])\n",
        "\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "nD3C2dJiC_Oh"
      },
      "id": "nD3C2dJiC_Oh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "EoXURLczDI8o"
      },
      "id": "EoXURLczDI8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dNPqQ9aANY2r"
      },
      "id": "dNPqQ9aANY2r"
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed hyperparameters\n",
        "d_model = 768  # Embedding dimension\n",
        "N = 7          # Number of encoder and decoder layers\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "id": "RUwTfLVGHNGD"
      },
      "id": "RUwTfLVGHNGD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "zXUCim7nMJlj"
      },
      "id": "zXUCim7nMJlj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replicating GPT2 with hyperparameters"
      ],
      "metadata": {
        "id": "mdsyjrm-Bflk"
      },
      "id": "mdsyjrm-Bflk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 768   # Embedding dimension to match GPT-2 small\n",
        "N = 12          # Number of layers; GPT-2 is decoder-only, but for a seq2seq model, this could apply to each part\n",
        "num_heads = 12  # Number of attention heads to match GPT-2 small\n",
        "d_ff = 3072     # Dimension of feed-forward networks, approximated to GPT-2's configuration\n",
        "max_seq_length = 1024 # Adjusted maximum sequence length to match GPT-2's capability\n",
        "dropout = 0.3  # Dropout rate increased to handle overfitting\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters for the training process\n",
        "batch_size = 512\n",
        "grad_clip = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "id": "MHAo8_CIAo80"
      },
      "id": "MHAo8_CIAo80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "CRA2JVIKHx4Z"
      },
      "id": "CRA2JVIKHx4Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 10 more epochs (total 20 epochs)\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "id": "US9ajc0pXLfY"
      },
      "id": "US9ajc0pXLfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "BlCv_3cbXP06"
      },
      "id": "BlCv_3cbXP06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Increasing number of attention heads"
      ],
      "metadata": {
        "id": "PhSXzwtE6JPr"
      },
      "id": "PhSXzwtE6JPr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed hyperparameters\n",
        "d_model = 528  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 12  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "id": "2qUUcRKBmLpO"
      },
      "id": "2qUUcRKBmLpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "IrN22MVf3Kdp"
      },
      "id": "IrN22MVf3Kdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA2a0Qk14vZN"
      },
      "id": "wA2a0Qk14vZN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}